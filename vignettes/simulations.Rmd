---
title: "Simulation Study"
bibliography: biblio.bib
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Simulations Study}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(knitr)
library(kableExtra)
```

# Simulation: Point Estimation

In this simulation study, we compare the performance of the three estimators considered in @guerrier2020accurate, namely the sample survey, $\bar{\pi}_2$, the MLE $\hat{\pi}_2$ as well as the *modified* MLE $\tilde{\pi}_2$. All simulation results are based on $B = 2500$ Monte-Carlo replications and we consider 12 simulation settings, which are presented in the table below:

```{r setting, fig.align='center'}
# Load package
library(CPreval)

# Number of Monte-Carlo simulations 
B = 2500

# Initial simulation seed
seed = 1832  

# Simulation settings
simu = data.frame(Simulation = 1:12,
                  pi2 = 100*rep(c(1.5/100, 1.5/10, 5/10), 4),
                  pi1 = 100*rep(c(1/100, 1/10, 3/10), 4),
                  n = rep(c(rep(1500, 3), rep(4000, 3)), 2),
                  alpha = 100*c(rep(0, 6), rep(0.01,6)),
                  beta = 100*c(rep(0, 6), rep(0.04,6)))

# Print table
kable(simu, col.names = c("Simulation ID",
                           "$\\pi_2$ (%)",
                           "$\\pi_1$ (%)",
                           "Sample size $n$",
                           "$\\alpha_1 = \\alpha_2$ (%)",
                           "$\\beta_1 = \\beta_2$ (%)")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

The code for the simulation is given here:

```{r simu, cache = TRUE, warning=FALSE}
# Initialisation     
res_survey = res_mle = res_mod_mle = matrix(NA, B, 12)
  
# Start Monte-Carlo 
for (j in 1:12){
  for (i in 1:B){
    X = sim_Rs(pi2 = simu$pi2[j]/100, pi1 = simu$pi1[j]/100, n = simu$n[j], seed = seed + i,
               alpha1 = simu$alpha[j]/100, alpha2 = simu$alpha[j]/100,
               beta1 = simu$beta[j]/100, beta2 = simu$beta[j]/100)
    res_survey[i,j] = survey_sample(R2 = X$R2, n = X$n, alpha2 = X$alpha2, beta2 = X$beta2)$estimate - simu$pi2[j]/100
    res_mod_mle[i,j] = modified_mle(R1 = X$R1, X$R2, n = X$n, pi1 = X$pi1, alpha1 = X$alpha1, alpha2 = X$alpha2, beta1 = X$beta1, beta2 = X$beta2)$estimate - simu$pi2[j]/100
    
    res_mle[i,j] = mle(R1 = X$R1, X$R2, n = X$n, pi1 = X$pi1, alpha1 = X$alpha1, alpha2 = X$alpha2, beta1 = X$beta1, beta2 = X$beta2)$estimate - simu$pi2[j]/100
  } 
}
```

In the figure below we compare the Root Mean Square Error (RMSE) for the three estimators in the 12 simulation settings.

```{r, fig.align='center', fig.width=9, fig.height=6}
RMSE = function(x)
  sqrt(mean(x)^2 + var(x))

cols = hcl(h = seq(15, 375, length = 4), l = 65, c = 100)[1:3]

rmse_survey = apply(res_survey, 2, RMSE)
rmse_mod_mle = apply(res_mod_mle, 2, RMSE)
rmse_mle = apply(res_mle, 2, RMSE)

plot(NA, xlim = c(1,12), ylim = c(0, 0.0185), xlab = "Simulation ID", 
     ylab = "RMSE", axes = FALSE)
rect(0.5, -0.01, 3.5, 1, col = "grey95", border = "grey95")
rect(6.5, -0.01, 9.5, 1, col = "grey95", border = "grey95")
text(2, 0.0005, "n = 1500")
text(5, 0.0005, "n = 4000")
text(8, 0.0005, "n = 1500")
text(11, 0.0005, "n = 4000")
text(3.5, 0.0145, "without measurement error")
text(9.5, 0.0145, "with measurement error")
abline(v = 6.5, lwd = 2, lty = 2)
axis(1, 1:12)
axis(2)
grid()
abline(v = 1:12, col = "grey80", lty = 3)
abline(h = (0:3)*0.005, col = "grey80", lty = 3)
box()
lines(1:12, rmse_survey, col = cols[1], pch = 15, type = "b")
lines(1:12, rmse_mod_mle, col = cols[2], pch = 16, type = "b")
lines(1:12, rmse_mle, col = cols[3], pch = 1, type = "b")
legend("topright", c("Survey Sample", "Modified MLE", "MLE"), col = cols,
       pch = c(15, 16, 1), lwd = 1, bty = "n")
```

lk

# References
